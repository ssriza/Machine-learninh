{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf46f410-5317-4ce2-8fb4-b52c8dc5d4f3",
   "metadata": {},
   "source": [
    "Task 1: Individual Exercise: Preprocess Your Own Text\n",
    "1. Collect Data: Copy a short paragraph (3-5 sentences) from a website or social media.\n",
    "2. Tokenize: Split the text into individual words.\n",
    "3. Remove Stop Words: Filter out common words like \"the,\" \"is,\" etc.\n",
    "4. Stem: Reduce words to their root forms.\n",
    "5. Lemmatize: Use POS tagging for context-aware lemmatization.\n",
    "6. Submit Results: Share your paragraph, final tokens, and a brief reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edba410f-d30f-44ac-80dd-a6213d110707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Birendra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Birendra\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Example 1: Original Text**\n",
      "AI is transforming industries rapidly! From healthcare to education, its applications are endless.\n",
      "\n",
      "**Example 2: Original Text**\n",
      "The sun rises in the east, setting a golden glow over thehorizon.\n",
      "\n",
      "**Example 3: Original Text**\n",
      "John's cat, which is black and white, loves to play with its toys.\n",
      "\n",
      "**Example 4: Original Text**\n",
      "Weather forecasts have improved dramatically due to AI-poweredmodels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.tokenize import word_tokenize # For splitting text into tokens\n",
    "from nltk.corpus import stopwords # For stop word removal\n",
    "from nltk.stem import PorterStemmer # For stemming words\n",
    "from nltk.stem import WordNetLemmatizer # For lemmatizing words\n",
    "from nltk import pos_tag, download # For POS tagging\n",
    "from nltk.corpus import wordnet # For WordNet integration(lemmatization)\n",
    "import re # For special character removal\n",
    "# Download necessary datasets if not already downloaded\n",
    "download('wordnet') # For lemmatization support\n",
    "download('averaged_perceptron_tagger') # For POS tagging\n",
    "# Sample text examples (students can replace these with their own)\n",
    "texts = [\n",
    " \"AI is transforming industries rapidly! From healthcare to education, its applications are endless.\",\n",
    " \"The sun rises in the east, setting a golden glow over thehorizon.\",\n",
    " \"John's cat, which is black and white, loves to play with its toys.\",\n",
    " \"Weather forecasts have improved dramatically due to AI-poweredmodels.\"\n",
    "]\n",
    "# Iterate over each example text\n",
    "for idx, text in enumerate(texts):\n",
    " print(f\"\\n**Example {idx + 1}: Original Text**\")\n",
    " print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15dad242-5577-4b0f-b773-690495f0a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens After Stop Word Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AI-poweredmodels', '.']\n"
     ]
    }
   ],
   "source": [
    " # Step 2a: Stop Word Removal - Removing common words that add no semantic value\n",
    " stop_words = set(stopwords.words('english')) # Load English stopwords\n",
    " tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    " print(\"\\nTokens After Stop Word Removal:\",tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b60d110-ee3e-4639-93ef-67e843986539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens After Special Character Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AIpoweredmodels']\n"
     ]
    }
   ],
   "source": [
    " # Step 2b: Special Character Removal - Removing punctuation or special symbols\n",
    " tokens_cleaned = [re.sub(r'[^\\w\\s]', '', word) for word in tokens_without_stopwords if re.sub(r'[^\\w\\s]', '', word)]\n",
    " print(\"\\nTokens After Special Character Removal:\", tokens_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83eff816-eeef-4093-ae22-ecd1298bcd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Tokens: ['weather', 'forecast', 'improv', 'dramat', 'due', 'aipoweredmodel']\n"
     ]
    }
   ],
   "source": [
    " # Step 3: Stemming - Reducing words to their root forms\n",
    " stemmer = PorterStemmer()\n",
    " stemmed_tokens = [stemmer.stem(word) for word in tokens_cleaned]\n",
    " print(\"\\nStemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae9e625-f1fb-43e1-8a84-c4498cd57d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Lemmatization with POS Tagging - Context-aware wordnormalization\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d55b50c-ff44-4ee4-b789-f8f8715d4600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens: ['Weather', 'forecasts', 'have', 'improved', 'dramatically', 'due', 'to', 'AI-poweredmodels', '.']\n",
      "\n",
      "Tokens After Stop Word Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AI-poweredmodels', '.']\n",
      "\n",
      "Tokens After Special Character Removal: ['Weather', 'forecasts', 'improved', 'dramatically', 'due', 'AIpoweredmodels']\n",
      "\n",
      "Stemmed Tokens: ['weather', 'forecast', 'improv', 'dramat', 'due', 'aipoweredmodel']\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Birendra/nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict.get(tag, wordnet.NOUN) \u001b[38;5;66;03m# Default to noun if tag is not found\u001b[39;00m\n\u001b[32m     23\u001b[39m  \u001b[38;5;66;03m# Apply lemmatization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m lemmatized_tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens_cleaned]\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLemmatized Tokens (with POS):\u001b[39m\u001b[33m\"\u001b[39m, lemmatized_tokens)\n\u001b[32m     26\u001b[39m  \u001b[38;5;66;03m# Final Reflection\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict.get(tag, wordnet.NOUN) \u001b[38;5;66;03m# Default to noun if tag is not found\u001b[39;00m\n\u001b[32m     23\u001b[39m  \u001b[38;5;66;03m# Apply lemmatization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m lemmatized_tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens_cleaned]\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mLemmatized Tokens (with POS):\u001b[39m\u001b[33m\"\u001b[39m, lemmatized_tokens)\n\u001b[32m     26\u001b[39m  \u001b[38;5;66;03m# Final Reflection\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mget_wordnet_pos\u001b[39m\u001b[34m(word)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_wordnet_pos\u001b[39m(word):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     tag = pos_tag([word])[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m].upper() \u001b[38;5;66;03m# Get the first letter of POS tag\u001b[39;00m\n\u001b[32m     20\u001b[39m     tag_dict = {\u001b[33m\"\u001b[39m\u001b[33mJ\u001b[39m\u001b[33m\"\u001b[39m: wordnet.ADJ, \u001b[33m\"\u001b[39m\u001b[33mN\u001b[39m\u001b[33m\"\u001b[39m: wordnet.NOUN, \u001b[33m\"\u001b[39m\u001b[33mV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     21\u001b[39m     wordnet.VERB, \u001b[33m\"\u001b[39m\u001b[33mR\u001b[39m\u001b[33m\"\u001b[39m: wordnet.ADV}\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict.get(tag, wordnet.NOUN)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cleanenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[39m, in \u001b[36mpos_tag\u001b[39m\u001b[34m(tokens, tagset, lang)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpos_tag\u001b[39m(tokens, tagset=\u001b[38;5;28;01mNone\u001b[39;00m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    166\u001b[39m \u001b[33;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     tagger = _get_tagger(lang)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cleanenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[39m, in \u001b[36m_get_tagger\u001b[39m\u001b[34m(lang)\u001b[39m\n\u001b[32m    108\u001b[39m     tagger = PerceptronTagger(lang=lang)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     tagger = PerceptronTagger()\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cleanenv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[39m, in \u001b[36mPerceptronTagger.__init__\u001b[39m\u001b[34m(self, load, lang)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_from_json(lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cleanenv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[39m, in \u001b[36mPerceptronTagger.load_from_json\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33meng\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    272\u001b[39m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     loc = find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtaggers/averaged_perceptron_tagger_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc + TAGGER_JSONS[lang][\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[32m    275\u001b[39m         \u001b[38;5;28mself\u001b[39m.model.weights = json.load(fin)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\cleanenv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Birendra/nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\anaconda3\\\\envs\\\\cleanenv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Birendra\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization - Splitting text into individual words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\nTokens:\", tokens)\n",
    " # Step 2a: Stop Word Removal - Removing common words that add no semantic value\n",
    "stop_words = set(stopwords.words('english')) # Load English stopwords\n",
    "tokens_without_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"\\nTokens After Stop Word Removal:\",tokens_without_stopwords)\n",
    " # Step 2b: Special Character Removal - Removing punctuation or special symbols\n",
    "tokens_cleaned = [re.sub(r'[^\\w\\s]', '', word) for word in tokens_without_stopwords if re.sub(r'[^\\w\\s]', '', word)]\n",
    "print(\"\\nTokens After Special Character Removal:\", tokens_cleaned)\n",
    " # Step 3: Stemming - Reducing words to their root forms\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens_cleaned]\n",
    "print(\"\\nStemmed Tokens:\", stemmed_tokens)\n",
    " # Step 4: Lemmatization with POS Tagging - Context-aware wordnormalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    " # Helper function to get WordNet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper() # Get the first letter of POS tag\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\":\n",
    "    wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if tag is not found\n",
    " # Apply lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in tokens_cleaned]\n",
    "print(\"\\nLemmatized Tokens (with POS):\", lemmatized_tokens)\n",
    " # Final Reflection\n",
    "print(f\"\\n**Final Results for Example {idx + 1}:**\")\n",
    "print(\"Original Tokens:\", tokens)\n",
    "print(\"Tokens After Stop Word Removal:\", tokens_without_stopwords)\n",
    "print(\"Tokens After Special Character Removal:\", tokens_cleaned)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"\\nTask Complete! Replace the example texts with your own for practice.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb891a-0075-4ff2-bc2b-06b1a78f8a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cleanenv]",
   "language": "python",
   "name": "conda-env-cleanenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
